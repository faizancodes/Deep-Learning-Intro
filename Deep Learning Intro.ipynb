{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc_pEhDYgCZG",
        "colab_type": "text"
      },
      "source": [
        "# **Preloaded data in Keras**\n",
        "\n",
        "> In Keras the MNIST dataset is preloaded in the form of four Numpy arrays and can be obtained with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVa7diebYdLn",
        "colab_type": "code",
        "outputId": "07d36249-bef0-4636-9809-0785c6f8cd26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDzfGgA2gRgA",
        "colab_type": "text"
      },
      "source": [
        "> x_train and y_train contain the training set, while x_test and y_test contain \n",
        "the test data. The images are encoded as Numpy arrays and their corresponding labels ranging from 0 to 9. Following the strategy of the post to gradually introduce the concepts of the subject, as we have indicated, we will not see yet how to separate a part of the training data to use them as Validation data. We will only take into account the training and test data.\n",
        "If we want to check what values we have loaded, we can choose any of the images of the MNIST set, for example image 8, and using the following Python code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l1jIXWtYx06",
        "colab_type": "code",
        "outputId": "bf3d2f64-0c09-439e-9bf2-f9cd84a7dce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.imshow(x_train[2], cmap=plt.cm.binary)\n",
        "print(y_train[2])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAWbyBEBfmwi",
        "colab_type": "text"
      },
      "source": [
        "# **Data representation in Keras**\n",
        "\n",
        "> Keras, which as we have seen uses a multidimensional array of Numpy as a basic data structure, calls this data structure a tensor. In short, we could say that a tensor has three main attributes:\n",
        "\n",
        "  > - ***Number of axes (Rank):*** a tensor containing a single number will be called scalar (or a 0-dimensional tensor, or tensor 0D). An array of numbers we call vector, or tensor 1D. An array of vectors will be a matrix, or 2D tensor. If we pack this matrix in a new array, we get a 3D tensor, which we can interpret visually as a cube of numbers. By packaging a 3D tensioner in an array, we can create a 4D tensioner, and so on. In the Python Numpy library this is called the tensor’s ndim.\n",
        "\n",
        " > - ***Shape:*** it is a tuple of integers that describe how many dimensions the tensor has along each axis. In the Numpy library this attribute is called shape.\n",
        "\n",
        " > - ***Data type:*** this attribute indicates the type of data that contains the tensor, which can be for example uint8, float32, float64, etc. In the Numpy library this attribute is called dtype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfMThWH6Y_vC",
        "colab_type": "code",
        "outputId": "116db208-8211-434d-84ff-c47e54c22d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(x_train.ndim)\n",
        "print(x_train.shape)\n",
        "print(x_train.dtype)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "(60000, 28, 28)\n",
            "uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyduOhgTfZQt",
        "colab_type": "text"
      },
      "source": [
        "# **Data normalization in Keras**\n",
        "\n",
        "\n",
        "> These MNIST images of 28×28 pixels are represented as an array of numbers whose values range from [0, 255] of type uint8. But it is usual to scale the input values of neural networks to certain ranges. In the example of this post the input values should be scaled to values of type float32 within the interval [0, 1]. We can achieve this transformation with the following lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDdlwSlGfVUf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANTdA4PAgpSW",
        "colab_type": "text"
      },
      "source": [
        "# **Converting 2D Tensor to 1D Vector**\n",
        "\n",
        "> On the other hand, to facilitate the entry of data into our neural network (we will see that in convolutionals it is not necessary) we must make a transformation of the tensor (image) from 2 dimensions (2D) to a vector of 1 dimension (1D). That is, the matrix of 28×28 numbers can be represented by a vector (array) of 784 numbers (concatenating row by row), which is the format that accepts as input a densely connected neural network like the one we will see in this post. In Python, converting every image of the MNIST dataset to avector with 784 components can be accomplished as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saIiIx-4g9wX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnWUSkSbhmTF",
        "colab_type": "text"
      },
      "source": [
        "> After executing these Python instructions, we can verify that x_train.shape takes the form of (60000, 784) and x_test.shape takes the form of (10000, 784), where the first dimension indexes the image and the second indexes the pixel in each image (now the intensity of the pixel is a value between 0 and 1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib_Ch4fChoJG",
        "colab_type": "code",
        "outputId": "1070bd1d-5c9e-4348-a9c4-06826aa8d93a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYXgExRmiCUZ",
        "colab_type": "text"
      },
      "source": [
        "> In addition to that, we have the labels for each input data (remember that in our case they are numbers between 0 and 9 that indicate which digit represents the image, that is, to which class is associated). In this example, and as we have already advanced, we will represent this label with a vector of 10 positions, where the position corresponding to the digit that represents the image contains a 1 and the remaining positions of the vector contain the value 0.\n",
        "\n",
        "> In this example we will use what is known as one-hot encoding, which we have already mentioned, which consists of transforming the labels into a vector of as many zeros as the number of different labels, and containing the value of 1 in the index that corresponds to the value of the label. Keras offers many support functions, including to_categorical to perform precisely this transformation, which we can import from keras.utils:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Nhh8BKkiHiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul30ApFsigiB",
        "colab_type": "text"
      },
      "source": [
        "> To see the effect of the transformation we can see the values before and after applying to_categorical :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c1XSlJZihbz",
        "colab_type": "code",
        "outputId": "240997a1-c186-4c8b-eddf-972414a58a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print('Y Train:', y_train[0])\n",
        "print('Y Test:', y_test[0])\n",
        "\n",
        "print('\\nY Train Shape:', y_train.shape)\n",
        "print('X Test Shape:', x_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y Train: 5\n",
            "Y Test: 7\n",
            "\n",
            "Y Train Shape: (60000,)\n",
            "X Test Shape: (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYFRY9u6kSb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0X4lx3vkazj",
        "colab_type": "code",
        "outputId": "9c7484b4-4ca0-42e0-f147-cd35480a6555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print('Y Train:', y_train[0])\n",
        "print('Y Test:', y_test[0])\n",
        "\n",
        "print('Y Train Shape:', y_train.shape)\n",
        "print('Y Test Shape:', y_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Y Train: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Y Test: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "Y Train Shape: (60000, 10)\n",
            "Y Test Shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOeu9ifplzGk",
        "colab_type": "text"
      },
      "source": [
        "# **Densely connected networks in Keras**\n",
        "\n",
        "> In this section, we will present how to specify in Keras the model that we have defined in the previous sections.\n",
        "\n",
        "**Sequential class in Keras**\n",
        "\n",
        "> The main data structure in Keras is the Sequential class, which allows the creation of a basic neural network. Keras also offers an API that allows implementing more complex models in the form of a graph that can have multiple inputs, multiple outputs, with arbitrary connections in between, but it is beyond the scope of this post.\n",
        "The Sequential class of the Keras library is a wrapper for the sequential neural network model that Keras offers and can be created in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScZQlTyAl7Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P19UJQO9l-mN",
        "colab_type": "text"
      },
      "source": [
        "> In this case, the model in Keras is considered as a sequence of layers and each of them gradually “distills” the input data to obtain the desired output. In Keras we can find all the required types of layers that can be easily added to the model through the add() method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyRebOcrmeMZ",
        "colab_type": "text"
      },
      "source": [
        "# **Defining the model**\n",
        "\n",
        "The construction in Keras of our model to recognize the images of digits could be the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H19j5lZqmgTw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.core import Dense, Activation\n",
        "\n",
        "model.add(Dense(10, activation='sigmoid', input_shape=(784,)))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtwTuiDRm3n6",
        "colab_type": "text"
      },
      "source": [
        "> Here, the neural network has been defined as a sequence of two layers that are densely connected (or fully connected), meaning that all the neurons in each layer are connected to all the neurons in the next layer. Visually we could represent it in the following way:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1400/1*c0fsl5kacv8nEwdIpji4bw.png)\n",
        "\n",
        "\n",
        "> In the previous code we explicitly express in the input_shape argument of the first layer what the input data is like: a tensor that indicates that we have 784 features of the model (in fact the tensor that is being defined is (None, 784,) as we will see more ahead).\n",
        "\n",
        "> A very interesting characteristic of the Keras library is that it will automatically deduce the shape of the tensors between layers after the first one. This means that the programmer only has to establish this information for the first of them. Also, for each layer we indicate the number of nodes that it has and the activation function that we will apply in it (in this example, sigmoid).\n",
        "\n",
        "> The second layer in this example is a softmax layer of 10 neurons, which means that it will return a matrix of 10 probability values representing the 10 possible digits (in general, the output layer of a classification network will have as many neurons as classes, except in a binary classification, where only one neuron is needed). Each value will be the probability that the image of the current digit belongs to each one of them.\n",
        "\n",
        "A very useful method that Keras provides to check the architecture of our model is summary():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UxkKydZns2I",
        "colab_type": "code",
        "outputId": "f9a25c22-d197-435f-8ef0-081fa400f910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                7850      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 7,960\n",
            "Trainable params: 7,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MXvfvOxo-AO",
        "colab_type": "text"
      },
      "source": [
        "> For our simple example, we see that it indicates that 7,960 parameters are required (column Param #), which correspond to 7,850 parameters to the first layer and 110 to the second.\n",
        "\n",
        "> In the first layer, for each neuron i (between 0 and 9) we require 784 parameters for the weights wij and therefore 10×784 parameters to store the weights of the 10 neurons. In addition to the 10 additional parameters for the 10 bj biases corresponding to each one of them. In the second layer, being a softmax function, it is required to connect all 10 neurons with the 10 neurons of the previous layer. Therefore 10x10 wi parameters are required and in addition 10 bj biases corresponding to each node.\n",
        "\n",
        "> The details of the arguments that we can indicate for the Dense layer can be found in the Keras manual. In our example, the most relevant ones appear. The first argument indicates the number of neurons in the layer; the following is the activation function that we will use in it. \n",
        "\n",
        "> The initialization of the weights is also often indicated as an argument of the Dense layers. The initial values must be adequate for the optimization problem to converge as quickly as possible. The various initialization options can also be found in the Keras manual.\n",
        "Basic steps to implement a neural network in Keras\n",
        "Next, we will present a brief description of the steps we must perform to implement a basic neural network and, in the following posts (soon), we will gradually introduce more details about each of these steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk-HZ9vwpKNW",
        "colab_type": "text"
      },
      "source": [
        "# **Configuration of the learning process**\n",
        "\n",
        "> From the Sequential model, we can define the layers in a simple way with the add() method, as we have advanced in the previous section. Once we have our model defined, we can configure how its learning process will be with the compile() method, with which we can specify some properties through method arguments.\n",
        "\n",
        "> The first of these arguments is the loss function that we will use to evaluate the degree of error between calculated outputs and the desired outputs of the training data. On the other hand, we specify an optimizer that, as we will see, is the way we have to specify the optimization algorithm that allows the neural network to calculate the weights of the parameters from the input data and the defined loss function. \n",
        "\n",
        "> And finally we must indicate the metric that we will use to monitor the learning process (and test) of our neural network. In this first example we will only consider the accuracy (fraction of images that are correctly classified). For example, in our case we can specify the following arguments in compile() method to test it on our computer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1m2COkEpxMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp17VEa8q_rM",
        "colab_type": "text"
      },
      "source": [
        "> In this example we specify that the loss function is categorical_crossentropy, the optimizer used is the stocastic gradient descent (sgd) and the metric is accuracy, with which we will evaluate the percentage of correct guesses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__gtVAqqrBcr",
        "colab_type": "text"
      },
      "source": [
        "# **Model training**\n",
        "\n",
        "Once our model has been defined and the learning method configured, it is ready to be trained. For this we can train or “adjust” the model to the training data available by invoking the fit() method of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSHkmRSVrF8W",
        "colab_type": "code",
        "outputId": "a94eee93-e104-4846-ea1b-ed5ae29918bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=200, epochs=20)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 1.1791 - accuracy: 0.7530\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 1.1225 - accuracy: 0.7640\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 1.0718 - accuracy: 0.7733\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 1.0262 - accuracy: 0.7817\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.9850 - accuracy: 0.7882\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.9477 - accuracy: 0.7949\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 12us/step - loss: 0.9139 - accuracy: 0.8014\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 12us/step - loss: 0.8830 - accuracy: 0.8066\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.8546 - accuracy: 0.8116\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.8286 - accuracy: 0.8165\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.8045 - accuracy: 0.8209\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.7823 - accuracy: 0.8252\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.7616 - accuracy: 0.8299\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.7423 - accuracy: 0.8335\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.7243 - accuracy: 0.8373\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.7074 - accuracy: 0.8407\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.6916 - accuracy: 0.8439\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.6767 - accuracy: 0.8472\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.6626 - accuracy: 0.8502\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 11us/step - loss: 0.6493 - accuracy: 0.8528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f873d2c90b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnsyfbQOrPAX",
        "colab_type": "text"
      },
      "source": [
        "> In the first two arguments we have indicated the data with which we will train the model in the form of Numpy arrays. The batch_size argument indicates the number of data that we will use for each update of the model parameters and with epochs we are indicating the number of times we will use all the data in the learning process. \n",
        "\n",
        "> This method finds the value of the parameters of the network through the iterative training algorithm that we mentioned. Roughly, in each iteration of this algorithm, training data from x_train, passes through the neural network (with the values that their parameters have at that moment), compares the obtained result with the expected one (indicated in y_train) and calculates the loss to guide the adjustment process of the model parameters, which intuitively consists of applying the optimizer specified above in the compile() method to calculate a new value of each one of the model parameters (weights and biases)in each iteration in such a way that the loss is reduced.\n",
        "\n",
        "> This is the method that, as we will see, may take longer and Keras allows us to see its progress using the verbose argument (by default, equal to 1), in addition to indicating an estimate of how long each epoch takes.\n",
        "\n",
        "> The fit() method allows many more arguments that have a very important impact on the learning outcome. Furthermore, this method returns a History object that we have omitted in this example. Its History.history attribute is the record of the loss values for the training data and other metrics in successive epochs, as well as other metrics for the validation data if they have been specified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vzVsMPAEEa2",
        "colab_type": "text"
      },
      "source": [
        "# **Model evaluation**\n",
        "\n",
        "> At this point, the neural network has been trained and its behavior with new test data can now be evaluated using the evaluation() method. This method returns two values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1jEvxWfrYAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "002df438-8c41-4f5b-a2c2-18222d02eef0"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 22us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BRkF8FWENWg",
        "colab_type": "text"
      },
      "source": [
        "> These values indicate how well or badly our model behaves with new data that it has never seen. These data have been stored in x_test and y_test when we have performed the mnist.load_data() and we pass them to the method as arguments. In the scope of this post we will only look at one of them, the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYS4JmACEMBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcc80fad-fe06-4e0d-f131-ba1fbed7aef7"
      },
      "source": [
        "print('Test accuracy:', test_acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8572999835014343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMsbXoK3FZ7t",
        "colab_type": "text"
      },
      "source": [
        "# **Generate predictions**\n",
        "\n",
        "> Finally, readers need to know how we can use the model trained in the previous section to make predictions. In our example, it consists in predict which digit represents an image. In order to do this, Keras supply the predict() method.\n",
        "\n",
        "> To test this method we can choose any element. For ease, let’s take one from the test dataset x_test. For example let’s choose the element 11 of this dataset x_test.\n",
        "\n",
        "> Before seeing the prediction, let’s see the image to be able to check ourselves if the model is making a correct prediction (before doing the previous reshape):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b5e5868-c438-408f-87de-556ca84d5da4",
        "id": "Cm4MtEEeHkeo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.imshow(x_test[11], cmap=plt.cm.binary)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f86b61106a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH7EHOExIgS0",
        "colab_type": "text"
      },
      "source": [
        "> Now let’s see that the predict() method of the model, executing the following code, correctly predicts the value that we have just estimated that it should predict.\n",
        "\n",
        "> The predict() method returns a vector with the predictions for the whole dataset elements. We can know which class gives the most probability of belonging by means of the argmax function of Numpy, which returns the index of the position that contains the highest value of the vector. Specifically, for item 11:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7817B6rGuCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29612b60-d5d5-46b1-fa80-df830bac756b"
      },
      "source": [
        "import numpy as np\n",
        "predictions = model.predict(x_test)\n",
        "np.argmax(predictions[11])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yndnGlpVI0jB",
        "colab_type": "text"
      },
      "source": [
        "> We can check it printing the vector returned by the method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IANNA9KI1vV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c9160c93-bc00-4812-c989-0ce8b3fc6d6d"
      },
      "source": [
        "print(predictions[11])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.22256902 0.0071131  0.10233448 0.01524724 0.10843305 0.0651271\n",
            " 0.38677177 0.00573045 0.06087364 0.0258002 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXv2YEdyI9Dd",
        "colab_type": "text"
      },
      "source": [
        "> We see that the highest value in the vector is in the position 6. We can also verify that the result of the prediction is a vector whose sum of all its components is equal to 1, as expected. For this we can use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydQXLWfKJD8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8815fb58-d4f7-4680-ddab-1d1476733620"
      },
      "source": [
        "np.sum(predictions[11])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    }
  ]
}
